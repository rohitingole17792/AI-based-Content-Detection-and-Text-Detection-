{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rohit\\anaconda3\\lib\\site-packages (4.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=pd.read_csv(r'C:\\Users\\rohit\\OneDrive\\Desktop\\pro\\final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quastion</th>\n",
       "      <th>answer</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Composers and works include Barbara Kolb , P...</td>\n",
       "      <td>human_ans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Sound mass is a term used to describe a type...</td>\n",
       "      <td>chat_gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Greco-Persian Wars (also often called th...</td>\n",
       "      <td>human_ans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Persian Wars were a series of conflicts ...</td>\n",
       "      <td>chat_gpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>what are add ons</td>\n",
       "      <td>['Plug-in (computing) , a piece of software wh...</td>\n",
       "      <td>human_ans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              quastion  \\\n",
       "0           0         what composer used sound mass   \n",
       "1           1         what composer used sound mass   \n",
       "2           2  where did the persian war take place   \n",
       "3           3  where did the persian war take place   \n",
       "4           4                      what are add ons   \n",
       "\n",
       "                                              answer      label  \n",
       "0  ['Composers and works include Barbara Kolb , P...  human_ans  \n",
       "1  ['Sound mass is a term used to describe a type...   chat_gpt  \n",
       "2  ['The Greco-Persian Wars (also often called th...  human_ans  \n",
       "3  ['The Persian Wars were a series of conflicts ...   chat_gpt  \n",
       "4  ['Plug-in (computing) , a piece of software wh...  human_ans  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quastion</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Composers and works include Barbara Kolb , P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Sound mass is a term used to describe a type...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Greco-Persian Wars (also often called th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Persian Wars were a series of conflicts ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>what are add ons</td>\n",
       "      <td>['Plug-in (computing) , a piece of software wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              quastion  \\\n",
       "0           0         what composer used sound mass   \n",
       "1           1         what composer used sound mass   \n",
       "2           2  where did the persian war take place   \n",
       "3           3  where did the persian war take place   \n",
       "4           4                      what are add ons   \n",
       "\n",
       "                                                text label  \n",
       "0  ['Composers and works include Barbara Kolb , P...     0  \n",
       "1  ['Sound mass is a term used to describe a type...     1  \n",
       "2  ['The Greco-Persian Wars (also often called th...     0  \n",
       "3  ['The Persian Wars were a series of conflicts ...     1  \n",
       "4  ['Plug-in (computing) , a piece of software wh...     0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.label=='human_ans'\n",
    "\n",
    "file_path['label'] = np.where(file_path['label']=='chat_gpt', '1', '0')\n",
    "\n",
    "file_path =file_path.rename({'answer': 'text'}, axis='columns')\n",
    "\n",
    "file_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6554 entries, 0 to 6553\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6554 non-null   int64 \n",
      " 1   quastion    6554 non-null   object\n",
      " 2   text        6554 non-null   object\n",
      " 3   label       6554 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 204.9+ KB\n"
     ]
    }
   ],
   "source": [
    "file_path[[\"label\"]] = file_path[[\"label\"]].apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "file_path.info()\n",
    "\n",
    "file_path.label\n",
    "\n",
    "text = file_path.text.values\n",
    "labels = file_path.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════╤═════════════╕\n",
      "│ Tokens          │   Token IDs │\n",
      "╞═════════════════╪═════════════╡\n",
      "│ [               │        1031 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ in              │        1999 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ science         │        2671 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ computing       │        9798 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ and             │        1998 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ engineering     │        3330 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ black           │        2304 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ is              │        2003 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ system          │        2291 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ which           │        2029 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ can             │        2064 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ be              │        2022 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ viewed          │        7021 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ in              │        1999 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ terms           │        3408 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ of              │        1997 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ its             │        2049 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ inputs          │       20407 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ and             │        1998 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ outputs         │       27852 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ (               │        1006 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ or              │        2030 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ transfer        │        4651 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ characteristics │        6459 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ )               │        1007 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ without         │        2302 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ any             │        2151 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ knowledge       │        3716 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ of              │        1997 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ its             │        2049 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ internal        │        4722 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ workings        │       24884 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ .               │        1012 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ its             │        2049 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ implementation  │        7375 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ is              │        2003 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ opaque          │       28670 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ (               │        1006 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ black           │        2304 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ )               │        1007 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ .               │        1012 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ term            │        2744 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ can             │        2064 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ be              │        2022 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ used            │        2109 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ to              │        2000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ refer           │        6523 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ to              │        2000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ many            │        2116 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ inner           │        5110 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ workings        │       24884 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ such            │        2107 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ as              │        2004 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ those           │        2216 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ of              │        1997 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ trans           │        9099 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##isto          │       20483 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##r             │        2099 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ an              │        2019 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ engine          │        3194 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ an              │        2019 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ algorithm       │        9896 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ human           │        2529 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ brain           │        4167 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ or              │        2030 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ an              │        2019 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ institution     │        5145 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ or              │        2030 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ government      │        2231 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ .               │        1012 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ nt              │       23961 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##o             │        2080 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ anal            │       20302 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##yse           │       23274 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ an              │        2019 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ open            │        2330 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ system          │        2291 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ with            │        2007 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ typical         │        5171 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ black           │        2304 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ approach        │        3921 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ only            │        2069 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ behavior        │        5248 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ of              │        1997 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ stimulus        │       19220 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ /               │        1013 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ response        │        3433 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ will            │        2097 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ be              │        2022 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ accounted       │       14729 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ for             │        2005 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ to              │        2000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ in              │        1999 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##fer           │        7512 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ (               │        1006 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ unknown         │        4242 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ )               │        1007 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ .               │        1012 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ usual           │        5156 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ representation  │        6630 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ of              │        1997 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ this            │        2023 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ black           │        2304 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ system          │        2291 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ is              │        2003 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ data            │        2951 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ flow            │        4834 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ diagram         │       16403 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ centered        │        8857 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ in              │        1999 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ .               │        1012 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ nt              │       23961 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##he            │        5369 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ opposite        │        4500 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ of              │        1997 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ black           │        2304 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ is              │        2003 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ system          │        2291 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ where           │        2073 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ the             │        1996 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ inner           │        5110 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ components      │        6177 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ or              │        2030 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ logic           │        7961 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ are             │        2024 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ available       │        2800 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ for             │        2005 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ inspection      │       10569 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ which           │        2029 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ is              │        2003 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ most            │        2087 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ commonly        │        4141 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ referred        │        3615 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ to              │        2000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ as              │        2004 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ white           │        2317 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ (               │        1006 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ sometimes       │        2823 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ also            │        2036 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ known           │        2124 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ as              │        2004 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ clear           │        3154 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ or              │        2030 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ a               │        1037 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ glass           │        3221 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ box             │        3482 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \\               │        1032 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ )               │        1007 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ .               │        1012 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ]               │        1033 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ,               │        1010 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ chat            │       11834 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##gp            │       21600 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ ##t             │        2102 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ _               │        1035 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ answers         │        6998 │\n",
      "├─────────────────┼─────────────┤\n",
      "│ \"               │        1000 │\n",
      "╘═════════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence():\n",
    "  '''Displays the tokens and respective IDs of a random text sample'''\n",
    "  index = random.randint(0,len(text)-1)\n",
    "  table = np.array([tokenizer.tokenize(text[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
    "  print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 64,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quastion</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Composers and works include Barbara Kolb , P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Sound mass is a term used to describe a type...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Greco-Persian Wars (also often called th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Persian Wars were a series of conflicts ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>what are add ons</td>\n",
       "      <td>['Plug-in (computing) , a piece of software wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              quastion  \\\n",
       "0           0         what composer used sound mass   \n",
       "1           1         what composer used sound mass   \n",
       "2           2  where did the persian war take place   \n",
       "3           3  where did the persian war take place   \n",
       "4           4                      what are add ons   \n",
       "\n",
       "                                                text  label  \n",
       "0  ['Composers and works include Barbara Kolb , P...      0  \n",
       "1  ['Sound mass is a term used to describe a type...      1  \n",
       "2  ['The Greco-Persian Wars (also often called th...      0  \n",
       "3  ['The Persian Wars were a series of conflicts ...      1  \n",
       "4  ['Plug-in (computing) , a piece of software wh...      0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 64,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\rohit\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for sample in text:\n",
    "  encoding_dict = preprocessing(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1031,  1000,  2304,  5958,  2003,  1996,  2154,  2206, 15060,\n",
       "         2154,  1999,  1996,  2142,  2163,  1010,  2411,  5240,  2004,  1996,\n",
       "         2927,  1997,  1996,  4234,  6023,  2161,  1012,  2023,  2001,  2579,\n",
       "         2000,  1037,  2047,  6034,  1999,  2249,  1010,  2043,  2195, 16629,\n",
       "         1006,  2164,  4539,  1010, 12849,  7317,  1005,  1055,  1010, 20914,\n",
       "         1005,  1055,  1010,  2190,  4965,  1010,  1998, 26892, 12718,  1007,\n",
       "         2441,  2012,  7090,   102])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_id[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒══════════╤═════════════╤══════════════════╕\n",
      "│ Tokens   │   Token IDs │   Attention Mask │\n",
      "╞══════════╪═════════════╪══════════════════╡\n",
      "│ [CLS]    │         101 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [        │        1031 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ '        │        1005 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ fruit    │        5909 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ cake     │        9850 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ (        │        1006 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ or       │        2030 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ fruit    │        5909 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##cake   │       17955 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ )        │        1007 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ is       │        2003 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ a        │        1037 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ cake     │        9850 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ made     │        2081 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ with     │        2007 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ chopped  │       24881 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ candi    │       27467 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##ed     │        2098 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ fruit    │        5909 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ and      │        1998 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ /        │        1013 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ or       │        2030 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ dried    │        9550 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ fruit    │        5909 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ,        │        1010 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ nuts     │       12264 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ,        │        1010 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ and      │        1998 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ spices   │       21729 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ,        │        1010 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ and      │        1998 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ (        │        1006 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ optional │       11887 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ##ly     │        2135 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ )        │        1007 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ soaked   │       13077 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ in       │        1999 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ spirits  │        8633 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ .        │        1012 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ '        │        1005 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ ]        │        1033 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [SEP]    │         102 │                1 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "├──────────┼─────────────┼──────────────────┤\n",
      "│ [PAD]    │           0 │                0 │\n",
      "╘══════════╧═════════════╧══════════════════╛\n"
     ]
    }
   ],
   "source": [
    "def print_rand_sentence_encoding():\n",
    "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
    "  index = random.randint(0, len(text)+1)\n",
    "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
    "  token_ids = [i.numpy() for i in token_id[index]]\n",
    "  attention = [i.numpy() for i in attention_masks[index]]\n",
    "\n",
    "  table = np.array([tokens, token_ids, attention]).T\n",
    "  print(tabulate(table, \n",
    "                 headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "batch_size = 64\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(0,len(labels)),\n",
    "    test_size = val_ratio,\n",
    "    shuffle = True,\n",
    "    stratify = labels)\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>quastion</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Composers and works include Barbara Kolb , P...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what composer used sound mass</td>\n",
       "      <td>['Sound mass is a term used to describe a type...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Greco-Persian Wars (also often called th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>where did the persian war take place</td>\n",
       "      <td>['The Persian Wars were a series of conflicts ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>what are add ons</td>\n",
       "      <td>['Plug-in (computing) , a piece of software wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                              quastion  \\\n",
       "0           0         what composer used sound mass   \n",
       "1           1         what composer used sound mass   \n",
       "2           2  where did the persian war take place   \n",
       "3           3  where did the persian war take place   \n",
       "4           4                      what are add ons   \n",
       "\n",
       "                                                text  label  \n",
       "0  ['Composers and works include Barbara Kolb , P...      0  \n",
       "1  ['Sound mass is a term used to describe a type...      1  \n",
       "2  ['The Greco-Persian Wars (also often called th...      0  \n",
       "3  ['The Persian Wars were a series of conflicts ...      1  \n",
       "4  ['Plug-in (computing) , a piece of software wh...      0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BertForSequenceClassification model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n",
    "# Run on GPU\n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [43:08<43:08, 2588.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.2217\n",
      "\t - Validation Accuracy: 0.9717\n",
      "\t - Validation Precision: 0.9598\n",
      "\t - Validation Recall: 0.9852\n",
      "\t - Validation Specificity: 0.9579\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [2:41:57<00:00, 4858.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t - Train loss: 0.0715\n",
      "\t - Validation Accuracy: 0.9627\n",
      "\t - Validation Precision: 0.9370\n",
      "\t - Validation Recall: 0.9911\n",
      "\t - Validation Specificity: 0.9346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
    "epochs = 2\n",
    "\n",
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  Hi..The pain you feel in the neck is due to muscle spasm...the spasm could occur due to irritation of neural structures or facet joints...In your case posture plays a major role in contributing to the pain...I suggest you toPut lot if icing frequently to the back of neck...A rolled towel under your neck curvature...will help you relax well...The more you hold your neck in proper posture the less pain you will have... You can modify your work station to maintain the posture...Try gentle stretches like bending your neck to touch your chin to chest...Bend your neck sideways till you feel stretch and not pain.. hold it in that stage...look up to the ceiling and push chin upwards...Once pain comes down I would like you to start your training for upper back...neck...like retraction work for your upper back..and pushing your neck with your hands clasped and neck pushing back...happening simultaneously....Hope this information help you to get better...Revert back incase you need further clarification..\n",
      "human_ans\n",
      "Predicted Class:  human_ans = human_ans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_sentence='Hi..The pain you feel in the neck is due to muscle spasm...the spasm could occur due to irritation of neural structures or facet joints...In your case posture plays a major role in contributing to the pain...I suggest you toPut lot if icing frequently to the back of neck...A rolled towel under your neck curvature...will help you relax well...The more you hold your neck in proper posture the less pain you will have... You can modify your work station to maintain the posture...Try gentle stretches like bending your neck to touch your chin to chest...Bend your neck sideways till you feel stretch and not pain.. hold it in that stage...look up to the ceiling and push chin upwards...Once pain comes down I would like you to start your training for upper back...neck...like retraction work for your upper back..and pushing your neck with your hands clasped and neck pushing back...happening simultaneously....Hope this information help you to get better...Revert back incase you need further clarification..'\n",
    "Actual_label='human_ans'\n",
    "# We need Token IDs and Attention Mask for inference on the new sentence\n",
    "test_ids = []\n",
    "test_attention_mask = []\n",
    "\n",
    "# Apply the tokenizer\n",
    "encoding = preprocessing(new_sentence, tokenizer)\n",
    "\n",
    "# Extract IDs and Attention Mask\n",
    "test_ids.append(encoding['input_ids'])\n",
    "test_attention_mask.append(encoding['attention_mask'])\n",
    "test_ids = torch.cat(test_ids, dim = 0)\n",
    "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "# Forward pass, calculate logit predictions\n",
    "with torch.no_grad():\n",
    "  output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "\n",
    "prediction = 'chat_gpt' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'human_ans'\n",
    "\n",
    "print('Input Sentence: ', new_sentence)\n",
    "print(Actual_label)\n",
    "print('Predicted Class: ',Actual_label,'=', prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
